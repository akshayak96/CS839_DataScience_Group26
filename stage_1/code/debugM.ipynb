{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------DT results----------------\n",
      "True Positives: 601\n",
      "True Negatives: 1732\n",
      "False Positives: 113\n",
      "False Negatives: 193\n",
      "Total number of data: 2639\n",
      "Accuracy: 0.884046987495\n",
      "Precision: 0.841736694678\n",
      "Recall: 0.756926952141\n",
      "F1: 0.797082228117\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "def accuracy(pred, ground_truth):\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    total = len(pred)\n",
    "    for i in range(0, total):\n",
    "        if pred[i] == 1 and ground_truth[i] == 1:\n",
    "            tp += 1\n",
    "        elif pred[i] == 1 and ground_truth[i] == 0:\n",
    "            fp += 1\n",
    "        elif pred[i] == 0 and ground_truth[i] == 0:\n",
    "            tn += 1\n",
    "        elif pred[i] == 0 and ground_truth[i] == 1:\n",
    "            fn += 1\n",
    "\n",
    "    accuracy = (tp + tn) / total\n",
    "\n",
    "    if tp == 0 and fp == 0:\n",
    "        precision = -1\n",
    "        recall = -1\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print(\"True Positives: {}\".format(tp))\n",
    "    print(\"True Negatives: {}\".format(tn))\n",
    "    print(\"False Positives: {}\".format(fp))\n",
    "    print(\"False Negatives: {}\".format(fn))\n",
    "    print(\"Total number of data: {}\".format(total))\n",
    "\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    print(\"Precision: {}\".format(precision))\n",
    "    print(\"Recall: {}\".format(recall))\n",
    "    print(\"F1: {}\".format(f1))\n",
    "    \n",
    "    return f1, precision, recall\n",
    "\n",
    "X_train = pd.read_csv('results/debug/P/features.csv', sep=',', index_col=0).values\n",
    "y_train = pd.read_csv('results/debug/P/labels.csv', sep=',', index_col=0, header=None).values.flatten()\n",
    "X_test = pd.read_csv('results/debug/Q/features.csv', sep=',', index_col=0).values\n",
    "y_test = pd.read_csv('results/debug/Q/labels.csv', sep=',', index_col=0, header=None).values.flatten()\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", random_state=0).fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print('-----------------DT results----------------')\n",
    "F1, Precision, Recall = accuracy(pred, y_test)\n",
    "\n",
    "# rule-based post processing\n",
    "'''cand = pd.read_csv('results/test/candidates.csv', sep=',', index_col=0, header=None).values.flatten()\n",
    "results = pd.DataFrame({'Candidates': cand})\n",
    "results['Labels'] = pd.Series(y_test, index=results.index)\n",
    "results['Prediction'] = pd.Series(pred, index=results.index)\n",
    "\n",
    "results.to_csv('results/debug/pred.csv', sep=',')\n",
    "fp = []\n",
    "for i in range(0, results.shape[0]):\n",
    "    if results.loc[i, 'Prediction'] == 1 and results.loc[i, 'Labels'] == 0:\n",
    "        fp.append(results.loc[i, 'Candidates'])\n",
    "fn = []\n",
    "for i in range(0, results.shape[0]):\n",
    "    if results.loc[i, 'Prediction'] == 0 and results.loc[i, 'Labels'] == 1:\n",
    "        fn.append(results.loc[i, 'Candidates'])\n",
    "fn_un = []\n",
    "for x in fn:\n",
    "    if x not in fn_un:\n",
    "        fn_un.append(x)\n",
    "\n",
    "pd.Series(fn_un).to_csv('results/debug/fn.csv', sep=',')\n",
    "pd.Series(fp).to_csv('results/debug/fp.csv', sep=',')\n",
    "rules = ['Federation', 'Field', 'Man', 'Hall', 'Associated', 'Press', 'District', 'Court', 'Park', 'Worlds', 'World', 'Indoor', 'Gardener', 'Sociedad', 'Board', 'Sharks', 'Federation', 'Evening', 'Chronicle', 'Souness', 'Newcastle', 'Former', 'Pompey', 'Evening', 'Chronicle', 'Souness', 'Welshman', 'Everything', 'Bridge', 'Indoor', 'Professional', 'Great', 'Everybody', 'White', 'Five', 'Six', 'Queens', 'Man', 'Executive', 'BBC', 'Everything', 'Agency', 'Wolves', 'Victory', 'Real ', 'History', 'Success', 'Striker', 'Firm', 'Field', 'Manager', 'Fatigue', 'Arch', 'Butler', 'Subtitute', 'BUPA']\n",
    "\n",
    "for i in range(0, results.shape[0]):\n",
    "    if len(results.loc[i, 'Candidates']) == 2 and results.loc[i, 'Prediction'] == 1 and results.loc[i, 'Labels'] == 0:\n",
    "        if results.loc[i, 'Candidates'][0] in rules or results.loc[i, 'Candidates'][1] in rules:\n",
    "            pred[i] = 0\n",
    "    elif len(results.loc[i, 'Candidates']) == 3 and results.loc[i, 'Prediction'] == 1 and results.loc[i, 'Labels'] == 0:\n",
    "        if results.loc[i, 'Candidates'][0] in rules or results.loc[i, 'Candidates'][1] in rules or results.loc[i, 'Candidates'][2] in rules:\n",
    "            pred[i] = 0\n",
    "    elif results.loc[i, 'Candidates'] in rules:\n",
    "            pred[i] = 0\n",
    "print('-----------------DT results----------------')\n",
    "F1, Precision, Recall = accuracy(pred, y_test) '''\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
