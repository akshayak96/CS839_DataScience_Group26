{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start with training data in set I.\n",
      "\n",
      "--------File--------> 1\n",
      "\n",
      "--------File--------> 2\n",
      "\n",
      "--------File--------> 3\n",
      "\n",
      "--------File--------> 4\n",
      "\n",
      "--------File--------> 5\n",
      "\n",
      "--------File--------> 6\n",
      "\n",
      "--------File--------> 7\n",
      "\n",
      "--------File--------> 8\n",
      "\n",
      "--------File--------> 9\n",
      "\n",
      "--------File--------> 10\n",
      "\n",
      "--------File--------> 11\n",
      "\n",
      "--------File--------> 12\n",
      "\n",
      "--------File--------> 13\n",
      "\n",
      "--------File--------> 14\n",
      "\n",
      "--------File--------> 15\n",
      "\n",
      "--------File--------> 16\n",
      "\n",
      "--------File--------> 17\n",
      "\n",
      "--------File--------> 18\n",
      "\n",
      "--------File--------> 19\n",
      "\n",
      "--------File--------> 20\n",
      "\n",
      "--------File--------> 21\n",
      "\n",
      "--------File--------> 22\n",
      "\n",
      "--------File--------> 23\n",
      "\n",
      "--------File--------> 24\n",
      "\n",
      "--------File--------> 25\n",
      "\n",
      "--------File--------> 26\n",
      "\n",
      "--------File--------> 27\n",
      "\n",
      "--------File--------> 28\n",
      "\n",
      "--------File--------> 29\n",
      "\n",
      "--------File--------> 30\n",
      "\n",
      "--------File--------> 31\n",
      "\n",
      "--------File--------> 32\n",
      "\n",
      "--------File--------> 33\n",
      "\n",
      "--------File--------> 34\n",
      "\n",
      "--------File--------> 35\n",
      "\n",
      "--------File--------> 36\n",
      "\n",
      "--------File--------> 37\n",
      "\n",
      "--------File--------> 38\n",
      "\n",
      "--------File--------> 39\n",
      "\n",
      "--------File--------> 40\n",
      "\n",
      "--------File--------> 41\n",
      "\n",
      "--------File--------> 42\n",
      "\n",
      "--------File--------> 43\n",
      "\n",
      "--------File--------> 44\n",
      "\n",
      "--------File--------> 45\n",
      "\n",
      "--------File--------> 46\n",
      "\n",
      "--------File--------> 47\n",
      "\n",
      "--------File--------> 48\n",
      "\n",
      "--------File--------> 49\n",
      "\n",
      "--------File--------> 50\n",
      "\n",
      "--------File--------> 51\n",
      "\n",
      "--------File--------> 52\n",
      "\n",
      "--------File--------> 53\n",
      "\n",
      "--------File--------> 54\n",
      "\n",
      "--------File--------> 55\n",
      "\n",
      "--------File--------> 56\n",
      "\n",
      "--------File--------> 57\n",
      "\n",
      "--------File--------> 58\n",
      "\n",
      "--------File--------> 59\n",
      "\n",
      "--------File--------> 60\n",
      "\n",
      "--------File--------> 61\n",
      "\n",
      "--------File--------> 62\n",
      "\n",
      "--------File--------> 63\n",
      "\n",
      "--------File--------> 64\n",
      "\n",
      "--------File--------> 65\n",
      "\n",
      "--------File--------> 66\n",
      "\n",
      "--------File--------> 67\n",
      "\n",
      "--------File--------> 68\n",
      "\n",
      "--------File--------> 69\n",
      "\n",
      "--------File--------> 70\n",
      "\n",
      "--------File--------> 71\n",
      "\n",
      "--------File--------> 72\n",
      "\n",
      "--------File--------> 73\n",
      "\n",
      "--------File--------> 74\n",
      "\n",
      "--------File--------> 75\n",
      "\n",
      "--------File--------> 76\n",
      "\n",
      "--------File--------> 77\n",
      "\n",
      "--------File--------> 78\n",
      "\n",
      "--------File--------> 79\n",
      "\n",
      "--------File--------> 80\n",
      "\n",
      "--------File--------> 81\n",
      "\n",
      "--------File--------> 82\n",
      "\n",
      "--------File--------> 83\n",
      "\n",
      "--------File--------> 84\n",
      "\n",
      "--------File--------> 85\n",
      "\n",
      "--------File--------> 86\n",
      "\n",
      "--------File--------> 87\n",
      "\n",
      "--------File--------> 88\n",
      "\n",
      "--------File--------> 89\n",
      "\n",
      "--------File--------> 90\n",
      "\n",
      "--------File--------> 91\n",
      "\n",
      "--------File--------> 92\n",
      "\n",
      "--------File--------> 93\n",
      "\n",
      "--------File--------> 94\n",
      "\n",
      "--------File--------> 95\n",
      "\n",
      "--------File--------> 96\n",
      "\n",
      "--------File--------> 97\n",
      "\n",
      "--------File--------> 98\n",
      "\n",
      "--------File--------> 99\n",
      "\n",
      "--------File--------> 100\n",
      "\n",
      "--------File--------> 101\n",
      "\n",
      "--------File--------> 102\n",
      "\n",
      "--------File--------> 103\n",
      "\n",
      "--------File--------> 104\n",
      "\n",
      "--------File--------> 105\n",
      "\n",
      "--------File--------> 106\n",
      "\n",
      "--------File--------> 107\n",
      "\n",
      "--------File--------> 108\n",
      "\n",
      "--------File--------> 109\n",
      "\n",
      "--------File--------> 110\n",
      "\n",
      "--------File--------> 111\n",
      "\n",
      "--------File--------> 112\n",
      "\n",
      "--------File--------> 113\n",
      "\n",
      "--------File--------> 114\n",
      "\n",
      "--------File--------> 115\n",
      "\n",
      "--------File--------> 116\n",
      "\n",
      "--------File--------> 117\n",
      "\n",
      "--------File--------> 118\n",
      "\n",
      "--------File--------> 119\n",
      "\n",
      "--------File--------> 120\n",
      "\n",
      "--------File--------> 121\n",
      "\n",
      "--------File--------> 122\n",
      "\n",
      "--------File--------> 123\n",
      "\n",
      "--------File--------> 124\n",
      "\n",
      "--------File--------> 125\n",
      "\n",
      "--------File--------> 126\n",
      "\n",
      "--------File--------> 127\n",
      "\n",
      "--------File--------> 128\n",
      "\n",
      "--------File--------> 129\n",
      "\n",
      "--------File--------> 130\n",
      "\n",
      "--------File--------> 131\n",
      "\n",
      "--------File--------> 132\n",
      "\n",
      "--------File--------> 133\n",
      "\n",
      "--------File--------> 134\n",
      "\n",
      "--------File--------> 135\n",
      "\n",
      "--------File--------> 136\n",
      "\n",
      "--------File--------> 137\n",
      "\n",
      "--------File--------> 138\n",
      "\n",
      "--------File--------> 139\n",
      "\n",
      "--------File--------> 140\n",
      "\n",
      "--------File--------> 141\n",
      "\n",
      "--------File--------> 142\n",
      "\n",
      "--------File--------> 143\n",
      "\n",
      "--------File--------> 144\n",
      "\n",
      "--------File--------> 145\n",
      "\n",
      "--------File--------> 146\n",
      "\n",
      "--------File--------> 147\n",
      "\n",
      "--------File--------> 148\n",
      "\n",
      "--------File--------> 149\n",
      "\n",
      "--------File--------> 150\n",
      "\n",
      "--------File--------> 151\n",
      "\n",
      "--------File--------> 152\n",
      "\n",
      "--------File--------> 153\n",
      "\n",
      "--------File--------> 154\n",
      "\n",
      "--------File--------> 155\n",
      "\n",
      "--------File--------> 156\n",
      "\n",
      "--------File--------> 157\n",
      "\n",
      "--------File--------> 158\n",
      "\n",
      "--------File--------> 159\n",
      "\n",
      "--------File--------> 160\n",
      "\n",
      "--------File--------> 161\n",
      "\n",
      "--------File--------> 162\n",
      "\n",
      "--------File--------> 163\n",
      "\n",
      "--------File--------> 164\n",
      "\n",
      "--------File--------> 165\n",
      "\n",
      "--------File--------> 166\n",
      "\n",
      "--------File--------> 167\n",
      "\n",
      "--------File--------> 168\n",
      "\n",
      "--------File--------> 169\n",
      "\n",
      "--------File--------> 170\n",
      "\n",
      "--------File--------> 171\n",
      "\n",
      "--------File--------> 172\n",
      "\n",
      "--------File--------> 173\n",
      "\n",
      "--------File--------> 174\n",
      "\n",
      "--------File--------> 175\n",
      "\n",
      "--------File--------> 176\n",
      "\n",
      "--------File--------> 177\n",
      "\n",
      "--------File--------> 178\n",
      "\n",
      "--------File--------> 179\n",
      "\n",
      "--------File--------> 180\n",
      "\n",
      "--------File--------> 181\n",
      "\n",
      "--------File--------> 182\n",
      "\n",
      "--------File--------> 183\n",
      "\n",
      "--------File--------> 184\n",
      "\n",
      "--------File--------> 185\n",
      "\n",
      "--------File--------> 186\n",
      "\n",
      "--------File--------> 187\n",
      "\n",
      "--------File--------> 188\n",
      "\n",
      "--------File--------> 189\n",
      "\n",
      "--------File--------> 190\n",
      "\n",
      "--------File--------> 191\n",
      "\n",
      "--------File--------> 192\n",
      "\n",
      "--------File--------> 193\n",
      "\n",
      "--------File--------> 194\n",
      "\n",
      "--------File--------> 195\n",
      "\n",
      "--------File--------> 196\n",
      "\n",
      "--------File--------> 197\n",
      "\n",
      "--------File--------> 198\n",
      "\n",
      "--------File--------> 199\n",
      "\n",
      "--------File--------> 200\n",
      "\n",
      "Continue with testing data in set J.\n",
      "\n",
      "--------File--------> 1\n",
      "\n",
      "--------File--------> 2\n",
      "\n",
      "--------File--------> 3\n",
      "\n",
      "--------File--------> 4\n",
      "\n",
      "--------File--------> 5\n",
      "\n",
      "--------File--------> 6\n",
      "\n",
      "--------File--------> 7\n",
      "\n",
      "--------File--------> 8\n",
      "\n",
      "--------File--------> 9\n",
      "\n",
      "--------File--------> 10\n",
      "\n",
      "--------File--------> 11\n",
      "\n",
      "--------File--------> 12\n",
      "\n",
      "--------File--------> 13\n",
      "\n",
      "--------File--------> 14\n",
      "\n",
      "--------File--------> 15\n",
      "\n",
      "--------File--------> 16\n",
      "\n",
      "--------File--------> 17\n",
      "\n",
      "--------File--------> 18\n",
      "\n",
      "--------File--------> 19\n",
      "\n",
      "--------File--------> 20\n",
      "\n",
      "--------File--------> 21\n",
      "\n",
      "--------File--------> 22\n",
      "\n",
      "--------File--------> 23\n",
      "\n",
      "--------File--------> 24\n",
      "\n",
      "--------File--------> 25\n",
      "\n",
      "--------File--------> 26\n",
      "\n",
      "--------File--------> 27\n",
      "\n",
      "--------File--------> 28\n",
      "\n",
      "--------File--------> 29\n",
      "\n",
      "--------File--------> 30\n",
      "\n",
      "--------File--------> 31\n",
      "\n",
      "--------File--------> 32\n",
      "\n",
      "--------File--------> 33\n",
      "\n",
      "--------File--------> 34\n",
      "\n",
      "--------File--------> 35\n",
      "\n",
      "--------File--------> 36\n",
      "\n",
      "--------File--------> 37\n",
      "\n",
      "--------File--------> 38\n",
      "\n",
      "--------File--------> 39\n",
      "\n",
      "--------File--------> 40\n",
      "\n",
      "--------File--------> 41\n",
      "\n",
      "--------File--------> 42\n",
      "\n",
      "--------File--------> 43\n",
      "\n",
      "--------File--------> 44\n",
      "\n",
      "--------File--------> 45\n",
      "\n",
      "--------File--------> 46\n",
      "\n",
      "--------File--------> 47\n",
      "\n",
      "--------File--------> 48\n",
      "\n",
      "--------File--------> 49\n",
      "\n",
      "--------File--------> 50\n",
      "\n",
      "--------File--------> 51\n",
      "\n",
      "--------File--------> 52\n",
      "\n",
      "--------File--------> 53\n",
      "\n",
      "--------File--------> 54\n",
      "\n",
      "--------File--------> 55\n",
      "\n",
      "--------File--------> 56\n",
      "\n",
      "--------File--------> 57\n",
      "\n",
      "--------File--------> 58\n",
      "\n",
      "--------File--------> 59\n",
      "\n",
      "--------File--------> 60\n",
      "\n",
      "--------File--------> 61\n",
      "\n",
      "--------File--------> 62\n",
      "\n",
      "--------File--------> 63\n",
      "\n",
      "--------File--------> 64\n",
      "\n",
      "--------File--------> 65\n",
      "\n",
      "--------File--------> 66\n",
      "\n",
      "--------File--------> 67\n",
      "\n",
      "--------File--------> 68\n",
      "\n",
      "--------File--------> 69\n",
      "\n",
      "--------File--------> 70\n",
      "\n",
      "--------File--------> 71\n",
      "\n",
      "--------File--------> 72\n",
      "\n",
      "--------File--------> 73\n",
      "\n",
      "--------File--------> 74\n",
      "\n",
      "--------File--------> 75\n",
      "\n",
      "--------File--------> 76\n",
      "\n",
      "--------File--------> 77\n",
      "\n",
      "--------File--------> 78\n",
      "\n",
      "--------File--------> 79\n",
      "\n",
      "--------File--------> 80\n",
      "\n",
      "--------File--------> 81\n",
      "\n",
      "--------File--------> 82\n",
      "\n",
      "--------File--------> 83\n",
      "\n",
      "--------File--------> 84\n",
      "\n",
      "--------File--------> 85\n",
      "\n",
      "--------File--------> 86\n",
      "\n",
      "--------File--------> 87\n",
      "\n",
      "--------File--------> 88\n",
      "\n",
      "--------File--------> 89\n",
      "\n",
      "--------File--------> 90\n",
      "\n",
      "--------File--------> 91\n",
      "\n",
      "--------File--------> 92\n",
      "\n",
      "--------File--------> 93\n",
      "\n",
      "--------File--------> 94\n",
      "\n",
      "--------File--------> 95\n",
      "\n",
      "--------File--------> 96\n",
      "\n",
      "--------File--------> 97\n",
      "\n",
      "--------File--------> 98\n",
      "\n",
      "--------File--------> 99\n",
      "\n",
      "--------File--------> 100\n",
      "\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import sys\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def feature_noun(pos_tag):\n",
    "    if pos_tag[1] == 'NN' or pos_tag[1] =='NNP':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def feature_adjective(pos_tag):\n",
    "    if pos_tag[1] == 'JJ':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def feature_verb(pos_tag):\n",
    "    if pos_tag[1] == 'VB' or pos_tag[1] == 'VBD' or pos_tag[1] == 'VBG' or pos_tag[1] == 'VBN':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def end_sent(idx):\n",
    "    if idx == 0:\n",
    "        return 1\n",
    "    elif idx != 0 and idx-3 == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def feat_specific(dest, gram):\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    " \n",
    "    if len(gram) == 2:\n",
    "        if gram[0].lower()  in dest or gram[1].lower()  in dest:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    elif len(gram) == 3:\n",
    "        if gram[0].lower() in dest or gram[1].lower()  in dest or gram[2].lower()  in dest:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if gram.lower() in dest:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "def num_capitals(gram):\n",
    "    string = ' '.join(gram)\n",
    "    return sum(1 for c in string if c.isupper())\n",
    "\n",
    "def num_vowels(gram):\n",
    "    string = ' '.join(gram)\n",
    "    vowels = 0\n",
    "    for i in string:\n",
    "        if(i.lower() == 'a' or i.lower() == 'e' or i.lower() == 'i' or i.lower() == 'o' or i.lower() == 'u'):\n",
    "            vowels=vowels+1\n",
    "    return vowels\n",
    "           \n",
    "def num_consonants(gram):\n",
    "    string = ' '.join(gram)\n",
    "    cons = list(set('bcdfghjklmnpqrstvwzyx'))\n",
    "    consonants = 0\n",
    "    for i in string:\n",
    "        if i.lower() in cons:\n",
    "            consonants = consonants + 1\n",
    "    return consonants\n",
    "           \n",
    "def has_digit(gram):\n",
    "    string = ' '.join(gram)\n",
    "    found = 0\n",
    "    for i in string:\n",
    "        if i.isdigit() is True:\n",
    "            found = 1\n",
    "            break\n",
    "           \n",
    "    if found == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def has_symbols(gram):\n",
    "    found  = 0\n",
    "    string = ' '.join(gram)\n",
    "    symbols = list(set('!,.@_!#$%^&*()<>?/\\}{~:]-'))\n",
    "    for i in string:\n",
    "        if i in symbols:\n",
    "            found = 1\n",
    "            break\n",
    "           \n",
    "    if found == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def remove_stopWords(one_grams, labels_ones, two_grams, labels_bi, thr_grams, labels_thr, features):\n",
    "    \n",
    "    candidates = []\n",
    "    labels = []\n",
    "    \n",
    "    indexes = []\n",
    "    stop_list = set(stopwords.words('english'))\n",
    "    for i in range(0, len(one_grams)):\n",
    "        if one_grams[i].lower() in stop_list:\n",
    "            indexes.append(i)\n",
    "    one_grams = [v for i,v in enumerate(one_grams) if i not in frozenset(indexes)]\n",
    "    labels_ones = [v for i,v in enumerate(labels_ones) if i not in frozenset(indexes)]\n",
    "    features.drop(indexes, inplace=True)\n",
    "    features.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    indexes = []\n",
    "    indexes_feat = []\n",
    "    for i in range(0, len(two_grams)):\n",
    "        if two_grams[i][0].lower() in stop_list or two_grams[i][1].lower() in stop_list:\n",
    "            indexes.append(i)\n",
    "            # ----- check if previous n-gram is empty now\n",
    "            if len(one_grams)!= 0:\n",
    "                indexes_feat.append(len(one_grams)+i-1)\n",
    "            else:\n",
    "                indexes_feat.append(i)\n",
    "            # ------------------------------------------\n",
    "    two_grams = [v for i,v in enumerate(two_grams) if i not in frozenset(indexes)]\n",
    "    labels_bi = [v for i,v in enumerate(labels_bi) if i not in frozenset(indexes)]\n",
    "    features.drop(indexes_feat, inplace=True)\n",
    "    features.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    indexes = []\n",
    "    indexes_feat = []\n",
    "    for i in range(0, len(thr_grams)):\n",
    "        if thr_grams[i][0].lower() in stop_list or thr_grams[i][1].lower() in stop_list or thr_grams[i][2].lower() in stop_list:\n",
    "            indexes.append(i)\n",
    "            indexes_feat.append(len(one_grams)+len(two_grams)+i-1)\n",
    "    thr_grams = [v for i,v in enumerate(thr_grams) if i not in frozenset(indexes)]\n",
    "    labels_thr = [v for i,v in enumerate(labels_thr) if i not in frozenset(indexes)] \n",
    "    features.drop(indexes_feat, inplace=True)\n",
    "    features.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    if len(one_grams) != 0:\n",
    "        for i in range(0, len(one_grams)):\n",
    "            candidates.append(one_grams[i])\n",
    "            labels.append(labels_ones[i])\n",
    "    if len(two_grams) != 0:  \n",
    "        for i in range(0, len(two_grams)):\n",
    "            candidates.append(two_grams[i])\n",
    "            labels.append(labels_bi[i])\n",
    "    if len(thr_grams) != 0:\n",
    "        for i in range(0, len(thr_grams)):\n",
    "            candidates.append(thr_grams[i])\n",
    "            labels.append(labels_thr[i])\n",
    "    \n",
    "    return candidates, labels, features\n",
    "    \n",
    "def remove_dupl_train(one_grams, labels_ones, two_grams, labels_bi, thr_grams, labels_thr, sent, features):\n",
    "    \n",
    "    \n",
    "    indexes = []\n",
    "    indexes_two = []\n",
    "    for i in range(0, len(one_grams)):\n",
    "        if len(two_grams) != 0:\n",
    "            if labels_ones[i] == 0:\n",
    "                for j in range(0, len(two_grams)):\n",
    "                    if one_grams[i] == two_grams[j][0] and labels_bi[j] == 1:\n",
    "                        indexes.append(i)\n",
    "                    elif one_grams[i] == two_grams[j][1] and labels_bi[j] == 1:\n",
    "                        indexes.append(i)\n",
    "        if len(thr_grams) != 0:\n",
    "            if labels_ones[i] == 0:\n",
    "                for j in range(0, len(thr_grams)):\n",
    "                    if one_grams[i] == thr_grams[j][0] and labels_thr[j] == 1:\n",
    "                        indexes.append(i)\n",
    "                    elif one_grams[i] == thr_grams[j][1] and labels_thr[j] == 1:\n",
    "                        indexes.append(i)\n",
    "                    elif one_grams[i] == thr_grams[j][2] and labels_thr[j] == 1:\n",
    "                        indexes.append(i)\n",
    "    for i in range(0, len(two_grams)):\n",
    "        if len(thr_grams) != 0:\n",
    "            if labels_bi[i] == 0:\n",
    "                for j in range(0, len(thr_grams)):\n",
    "                    if (two_grams[i][0] == thr_grams[j][0] or two_grams[i][0] == thr_grams[j][1]) and labels_thr[j] == 1:\n",
    "                        indexes_two.append(i)\n",
    "                    elif (two_grams[i][1] == thr_grams[j][1] or two_grams[i][0] == thr_grams[j][2]) and labels_thr[j] == 1:\n",
    "                        indexes_two.append(i)\n",
    "\n",
    "    one_grams = [v for i,v in enumerate(one_grams) if i not in frozenset(indexes)]\n",
    "    labels_ones = [v for i,v in enumerate(labels_ones) if i not in frozenset(indexes)]\n",
    "    features.drop(indexes, inplace=True)\n",
    "    features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    two_grams = [v for i,v in enumerate(two_grams) if i not in frozenset(indexes_two)]\n",
    "    labels_bi = [v for i,v in enumerate(labels_bi) if i not in frozenset(indexes_two)]\n",
    "    indexes_two = [idx+len(one_grams)-1 for idx in indexes_two]\n",
    "    features.drop(indexes_two, inplace=True)\n",
    "    features.reset_index(drop=True, inplace=True)\n",
    "    return one_grams, labels_ones, two_grams, labels_bi, thr_grams, labels_thr, features\n",
    "\n",
    "def extract_candidates(tok, sent, pos_tags):\n",
    "    \n",
    "    dest = pd.read_csv('destinations.csv', sep=',')\n",
    "    dest = dest.iloc[:, 1].tolist()\n",
    "    dest = [w.lower() for w in dest]\n",
    "    dest = dest + ['hellenic','munich', 'lisburn', 'middlesbrough', 'scot', 'exeter', 'caledonian','ibrox', 'charlton' ,'dutch' ,'african', 'greece', 'greek', 'athens', 'netherlands', 'italy', 'south', 'korea', 'europe','spain', 'france', 'finland', 'sweden', 'asia', 'continental', 'british', 'irishman','britain', 'uk', 'usa', 'london', 'manchester', 'egeland', 'central', 'america', 'north''east', 'west', 'thailand', 'indonesia', 'india', 'sri', \"lan\", 'world', 'maldives','islands', 'malaysia', 'zurich', 'german', 'swiss', 'hanover', 'western', 'eastern','australia', 'australian', 'cambridge', 'england', 'jamaica', 'jamaican','hill', 'belgrade', 'troy', 'australian', 'las', 'vegas', 'california', 'los', 'angeles''russia', 'russian', 'china', 'italy', 'siberian', 'sudan', 'vietnam', 'indian','new', 'delhi', 'valley', 'moscow', 'boston', 'european', 'albania', 'albanian', 'balkans','atlantic', 'euro',  'macedonia', 'bulgaria', 'sea', 'adriatic', 'vlore','burgas', 'bulgarian', 'macedonian', 'turkish', 'caspioan', 'york','maryland', 'washington', 'county', 'chelsea', 'united', 'zealand', 'paris','miami', 'florida', 'hoolywood', 'denmark', 'angola', 'korea','germany', 'palestinian', 'israel', 'utah', 'french', 'france', 'irish','northwest', 'canada', 'ontario', 'africa', 'jamaica', 'nation', 'nations','international', 'iraq', 'city', 'town', 'country', 'czech', 'republic','portugal', 'ohio', 'river', 'mountain', 'canal', 'canals', 'chicago', 'scadinavian''indonesia', 'egypt', 'earth', 'norway', 'global', 'scotland', 'taiwan','japanese', 'rotterdam', 'scotland', 'sydney', 'houston', 'midlands', 'latin','kingdom', 'mississipi', 'chinese', 'mongolia', 'deutsche', 'central','brixton', 'bristol', 'ireland', 'national', 'cambridge', 'australasia','japan', 'pacific', 'chile', 'federal', 'boulevard', 'world','philadelphia', 'disney', 'venezuela', 'argentina', 'brazil', 'aires', 'buenos','venezuelan', 'dublin', 'frances', 'mexican', 'oklahoma', 'michigan', 'street','edinburgh', 'kenyan', 'moroccan', 'ethiopian', 'swede', 'dutchman', 'briton']\n",
    "    pd.Series(dest).to_csv('destinations.csv', sep=',')\n",
    "    \n",
    "    sport = ['chelsea', 'bayern','sprint', 'live', 'dinamo', 'training', 'grand','juventus','sprinter', 'marathon', 'atletico','footballers','newcastle' ,'anti-doping' , 'champion', 'championship', 'manchester','boxing', 'reebok', 'celtics', 'celtic', 'games', 'game', 'sport', 'sports', 'inter', 'iaaf', 'arena', 'crystal', 'warriors', 'athletics', 'football', 'carling', 'cup', 'fa', 'fifa', 'liverpool','premier', 'premiership', 'league', 'union', 'trafford', 'olympic', 'olympics','olympian', 'palace', 'arsenal', 'arsene', 'tottenham', 'athletico', 'madrid', 'primera', 'liga','daily', 'baros', 'millenium', 'stadium', 'brom', 'reds', 'blues', 'aston', 'villa', 'cross', 'challenge', 'court']\n",
    "    date = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'january', 'february', 'march','april', 'may', 'june', 'july', 'september', 'october', 'november', 'december', 'day', 'month', 'year','eve', 'christmas']\n",
    "    prefix = ['mr', 'mrs', 'ms', 'miss', 'mx', 'master', 'maid', 'madam','professor', 'prof', 'chief', 'phd', 'dphil', 'dr', 'doc','advocate', 'economist', 'chancellor', 'lord', 'sir', 'dame','jr', 'duke', 'don', 'president', 'prince', 'king', 'emperor','queen', 'leader', 'emperor', 'empress', 'lord', 'tsar', 'tsarina','count', 'countess', 'duchess', 'vice', 'ambassador', 'secretary','governor', 'mayor', 'councillor', 'speaker', 'doctor', 'representative','senator', 'hon', 'aunt', 'uncle', 'prelate', 'premier', 'Prefect', 'burgess''envoy', 'provost', 'priest','father', 'catholic', 'cardinal', 'dean', 'minister','reader', 'teacher', 'christ', 'major', 'general', 'officer', 'captain', 'engineer','principal', 'coach', 'player', 'citizen', 'male', 'female', 'mother', 'father', 'executive','founder', 'lawyer', 'former', 'challenger', 'laureate', 'pioneer', 'chairman', 'boss', 'member','challenger', 'minister', 'explained', 'boss','member', 'analyst', 'commissioner', 'explains','commented', 'creator', 'director',  'co-chairman', 'former', 'manager', 'who', 'designer','told', 'star', 'writer', 'ruled', 'actor', 'spokesman', 'nominee', 'actress', 'daughter','winner', 'artist', 'anchor', 'model', 'comedian', 'spokeswoman', 'singer','member', 'prop']\n",
    "    f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16, f17 = ([] for i in range(18))\n",
    "    \n",
    "    one_grams = []\n",
    "    two_grams = []\n",
    "    thr_grams = []\n",
    "    \n",
    "    labels_ones= []\n",
    "    labels_bi = []\n",
    "    labels_thr = []\n",
    "    \n",
    "    for i in range(0, len(tok)):\n",
    "        if str.isupper(tok[i][0]) is True:\n",
    "            one_grams.append(tok[i])\n",
    "            # ------create feautures --------\n",
    "            f0.append(1)\n",
    "            f1.append(0)\n",
    "            f2.append(0)\n",
    "            f3.append(feature_noun(pos_tags[i]))\n",
    "            f4.append(feature_adjective(pos_tags[i]))\n",
    "            f5.append(feature_verb(pos_tags[i]))\n",
    "            f6.append(end_sent(i))\n",
    "            \n",
    "            if i != len(tok)-1 and tok[i+1] == \"'s\":\n",
    "                f7.append(1)\n",
    "            elif i != len(tok)-4 and tok[i+1] == \"<\" and tok[i+4] == \"'s\":\n",
    "                f7.append(1)\n",
    "            else:\n",
    "                f7.append(0)\n",
    "            \n",
    "            if i != 0 and tok[i-1] in prefix:\n",
    "                f8.append(1)\n",
    "            elif i != 0 and tok[i-1] == \">\" and i-4>=0 and tok[i-4] in prefix:\n",
    "                f8.append(1)\n",
    "            else:\n",
    "                f8.append(0)\n",
    "            \n",
    "            f9.append(feat_specific(dest, tok[i]))\n",
    "            f10.append(feat_specific(date, tok[i]))\n",
    "            f11.append(num_capitals(tok[i]))\n",
    "            f12.append(num_vowels(tok[i]))\n",
    "            f13.append(num_consonants(tok[i]))\n",
    "            f14.append(has_symbols(tok[i]))\n",
    "            f15.append(has_digit(tok[i]))\n",
    "            \n",
    "            verbs = [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "            nouns = [\"NN\", \"NNP\"]\n",
    "            if i != len(tok)-1 and pos_tags[i+1][1] in verbs and pos_tags[i][1] in nouns:\n",
    "                f16.append(1)\n",
    "            elif i != len(tok)-2 and pos_tags[i+1][1] == 'TO' and pos_tags[i+2][1] in verbs and pos_tags[i][1] in nouns:\n",
    "                f16.append(1)\n",
    "            elif i != len(tok)-2 and pos_tags[i+1][1] == 'MD' and pos_tags[i+2][1] in verbs and pos_tags[i][1] in nouns:\n",
    "                f16.append(1)\n",
    "            elif i != len(tok)-4 and tok[i+1] == '<' and pos_tags[i][1] in nouns and pos_tags[i+4][1] in verbs:\n",
    "                f16.append(1)\n",
    "            elif i != len(tok)-5 and tok[i+1] == '<' and pos_tags[i+4][1] == 'TO' and pos_tags[i+5][1] in verbs and pos_tags[i][1] in nouns:\n",
    "                f16.append(1)\n",
    "            elif i != len(tok)-5 and tok[i+1] == '<' and pos_tags[i+4][1] == 'MD' and pos_tags[i+5][1] in verbs and pos_tags[i][1] in nouns:\n",
    "                f16.append(1)\n",
    "            else:\n",
    "                f16.append(0)\n",
    "            f17.append(feat_specific(sport, tok[i]))\n",
    "            # -------------------------------\n",
    "            if i!=0 and tok[i-1] == '>' and tok[i+1] == '<':\n",
    "                labels_ones.append(1)\n",
    "            else:\n",
    "                labels_ones.append(0)\n",
    "            \n",
    "    bigrams = list(ngrams(tok, 2))\n",
    "    for i in range(0, len(bigrams)):\n",
    "        if str.isupper(bigrams[i][0][0]) is True and str.isupper(bigrams[i][1][0]) is True:\n",
    "            two_grams.append(bigrams[i])\n",
    "            \n",
    "            # ------create feautures --------\n",
    "            f0.append(0)\n",
    "            f1.append(1)\n",
    "            f2.append(0)\n",
    "            for m in range(0, len(tok)):\n",
    "                if m != len(tok)-1:\n",
    "                    if tok[m] == bigrams[i][0] and tok[m+1] == bigrams[i][1]:\n",
    "                        f3.append(feature_noun(pos_tags[m]))\n",
    "                        f4.append(feature_adjective(pos_tags[m]))\n",
    "                        f5.append(feature_verb(pos_tags[m]))\n",
    "                        f6.append(end_sent(m))\n",
    "                        \n",
    "                        if i != len(tok)-2 and tok[i+2] == \"'s\":\n",
    "                            f7.append(1)\n",
    "                        elif i != len(tok)-5 and tok[i+2] == \"<\" and tok[i+5] == \"'s\":\n",
    "                            f7.append(1)\n",
    "                        else:\n",
    "                            f7.append(0)\n",
    "                            \n",
    "                        if i != 0 and tok[i-1] in prefix:\n",
    "                            f8.append(1)\n",
    "                        elif i != 0 and tok[i-1] == \">\" and i-4>=0 and tok[i-4] in prefix:\n",
    "                            f8.append(1)\n",
    "                        else:\n",
    "                            f8.append(0)\n",
    "                        \n",
    "                        f9.append(feat_specific(dest, bigrams[i]))\n",
    "                        f10.append(feat_specific(date, bigrams[i]))\n",
    "                        f11.append(num_capitals(bigrams[i]))\n",
    "                        f12.append(num_vowels(bigrams[i]))\n",
    "                        f13.append(num_consonants(bigrams[i]))\n",
    "                        f14.append(has_symbols(bigrams[i]))\n",
    "                        f15.append(has_digit(bigrams[i]))\n",
    "                        \n",
    "                        verbs = [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "                        nouns = [\"NN\", \"NNP\"]\n",
    "                        if i != len(tok)-2 and pos_tags[i+2][1] in verbs and pos_tags[i][1] in nouns:\n",
    "                            f16.append(1)\n",
    "                        elif i != len(tok)-3 and pos_tags[i+2][1] == 'TO' and pos_tags[i+3][1] in verbs and pos_tags[i][1] in nouns:\n",
    "                            f16.append(1)\n",
    "                        elif i != len(tok)-3 and pos_tags[i+2][1] == 'MD' and pos_tags[i+3][1] in verbs and pos_tags[i][1] in nouns:\n",
    "                            f16.append(1)\n",
    "                        elif i != len(tok)-5 and tok[i+2] == '<' and pos_tags[i][1] in nouns and pos_tags[i+5][1] in verbs:\n",
    "                            f16.append(1)\n",
    "                        elif i != len(tok)-6 and tok[i+2] == '<' and pos_tags[i+5][1] == 'TO' and pos_tags[i+6][1] in verbs and pos_tags[i][1] in nouns:\n",
    "                            f16.append(1)\n",
    "                        elif i != len(tok)-6 and tok[i+2] == '<' and pos_tags[i+5][1] == 'MD' and pos_tags[i+6][1] in verbs and pos_tags[i][1] in nouns:\n",
    "                            f16.append(1)\n",
    "                        else:\n",
    "                            f16.append(0)\n",
    "                        \n",
    "                        f17.append(feat_specific(sport, bigrams[i]))\n",
    "                        \n",
    "                        break\n",
    "            \n",
    "            # -------------------------------\n",
    "\n",
    "            if i!=0 and tok[i-1] == '>' and tok[i+2] == '<':\n",
    "                labels_bi.append(1)\n",
    "            else:\n",
    "                labels_bi.append(0)\n",
    "                \n",
    "    threegrams = list(ngrams(tok, 3))\n",
    "    for i in range(0, len(threegrams)):\n",
    "        if str.isupper(threegrams[i][0][0]) is True and str.isupper(threegrams[i][1][0]) is True and str.isupper(threegrams[i][2][0]) is True:\n",
    "            thr_grams.append(threegrams[i])\n",
    "            \n",
    "            # ------create feautures --------\n",
    "            f0.append(0)\n",
    "            f1.append(0)\n",
    "            f2.append(1)\n",
    "            for m in range(0, len(tok)):\n",
    "                if m != len(tok)-2:\n",
    "                    if tok[m] == threegrams[i][0] and tok[m+1] == threegrams[i][1] and tok[m+2] == threegrams[i][2]:\n",
    "                        f3.append(feature_noun(pos_tags[m]))\n",
    "                        f4.append(feature_adjective(pos_tags[m]))\n",
    "                        f5.append(feature_verb(pos_tags[m]))\n",
    "                        f6.append(end_sent(m))\n",
    "                        if i != len(tok)-3 and tok[i+3] == \"'s\":\n",
    "                            f7.append(1)\n",
    "                        elif i != len(tok)-6 and tok[i+3] == \"<\" and tok[i+6] == \"'s\":\n",
    "                            f7.append(1)\n",
    "                        else:\n",
    "                            f7.append(0)\n",
    "                        \n",
    "                        if i != 0 and tok[i-1] in prefix:\n",
    "                            f8.append(1)\n",
    "                        elif i != 0 and tok[i-1] == \">\" and i-4>=0 and tok[i-4] in prefix:\n",
    "                            f8.append(1)\n",
    "                        else:\n",
    "                            f8.append(0)\n",
    "                        f9.append(feat_specific(dest, threegrams[i])) \n",
    "                        f10.append(feat_specific(date, threegrams[i]))\n",
    "                        f11.append(num_capitals(threegrams[i]))\n",
    "                        f12.append(num_vowels(threegrams[i]))\n",
    "                        f13.append(num_consonants(threegrams[i]))\n",
    "                        f14.append(has_symbols(threegrams[i]))\n",
    "                        f15.append(has_digit(threegrams[i]))\n",
    "                        \n",
    "                        \n",
    "                        if i != len(tok)-3 and pos_tags[i+3][1] in verbs and pos_tags[i][1] in nouns:\n",
    "                            f16.append(1)\n",
    "                        elif i != len(tok)-4 and pos_tags[i+3][1] == 'TO' and pos_tags[i+4][1] in verbs and pos_tags[i][1] in nouns:\n",
    "                            f16.append(1)\n",
    "                        elif i != len(tok)-4 and pos_tags[i+3][1] == 'MD' and pos_tags[i+4][1] in verbs and pos_tags[i][1] in nouns:\n",
    "                            f16.append(1)\n",
    "                        elif i != len(tok)-6 and tok[i+3] == '<' and pos_tags[i][1] in nouns and pos_tags[i+6][1] in verbs:\n",
    "                            f16.append(1)\n",
    "                        elif i != len(tok)-7 and tok[i+3] == '<' and pos_tags[i+5][1] == 'TO' and pos_tags[i+7][1] in verbs and pos_tags[i][1] in nouns:\n",
    "                            f16.append(1)\n",
    "                        elif i != len(tok)-7 and tok[i+3] == '<' and pos_tags[i+5][1] == 'MD' and pos_tags[i+7][1] in verbs and pos_tags[i][1] in nouns:\n",
    "                            f16.append(1)\n",
    "                        else:\n",
    "                            f16.append(0)\n",
    "                            \n",
    "                        f17.append(feat_specific(sport, threegrams[i]))\n",
    "                        break\n",
    "            # -------------------------------\n",
    "            \n",
    "            if i!=0 and tok[i-1] == '>' and tok[i+3] == '<':\n",
    "                labels_thr.append(1)\n",
    "            else:\n",
    "                labels_thr.append(0)\n",
    "\n",
    "    \n",
    "    features = pd.DataFrame({'f0': f0})\n",
    "    features['f1'] = pd.Series(f1, index=features.index)\n",
    "    features['f2'] = pd.Series(f2, index=features.index)\n",
    "    features['f3'] = pd.Series(f3, index=features.index)\n",
    "    features['f4'] = pd.Series(f4, index=features.index)\n",
    "    features['f5'] = pd.Series(f5, index=features.index)\n",
    "    features['f6'] = pd.Series(f6, index=features.index)\n",
    "    features['f7'] = pd.Series(f7, index=features.index)\n",
    "    features['f8'] = pd.Series(f8, index=features.index)\n",
    "    features['f9'] = pd.Series(f9, index=features.index)\n",
    "    features['f10'] = pd.Series(f10, index=features.index)\n",
    "    features['f11'] = pd.Series(f11, index=features.index)\n",
    "    features['f12'] = pd.Series(f12, index=features.index)\n",
    "    features['f13'] = pd.Series(f13, index=features.index)\n",
    "    features['f14'] = pd.Series(f14, index=features.index)\n",
    "    features['f15'] = pd.Series(f15, index=features.index)\n",
    "    features['f16'] = pd.Series(f16, index=features.index)\n",
    "    features['f17'] = pd.Series(f17, index=features.index)\n",
    "    \n",
    "    one_grams, labels_ones, two_grams, labels_bi, thr_grams, labels_thr, features = remove_dupl_train(one_grams, labels_ones, two_grams, labels_bi, thr_grams, labels_thr, sent, features)\n",
    "    sent_cand, labels, features = remove_stopWords(one_grams, labels_ones, two_grams, labels_bi, thr_grams, labels_thr, features)\n",
    "\n",
    "    return sent_cand, labels, features\n",
    "\n",
    "\n",
    "def process_sentence(sent):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    sent_cand, labels, sent_features = extract_candidates(tokens, sent, pos_tags)\n",
    "    return sent_cand, labels, sent_features\n",
    "    \n",
    "\n",
    "def read_files(type_file):\n",
    "    path = \"/Users/elenamilkai/Desktop/project-stage1/data/\"+type_file+'Y/'\n",
    "    #path = \"/Users/elenamilkai/Desktop/project-stage1/data/\"+type_file+'/'\n",
    "    candidates = []\n",
    "    labels_all = []\n",
    "    features_all = pd.DataFrame(columns=['f0','f1','f2','f3'])\n",
    "    \n",
    "    num_files = 0\n",
    "    for filename in os.listdir(path):\n",
    "        if not filename.startswith('.'):\n",
    "            num_files += 1\n",
    "            print('--------File--------> '+str(num_files)+'\\n')\n",
    "            with open(path + filename, 'r') as myfile:\n",
    "                data = myfile.read().replace('\\n', ' ')\n",
    "                sentences = nltk.tokenize.sent_tokenize(data)\n",
    "               \n",
    "                for j in range(0, len(sentences)):\n",
    "                    cand_sent, labels, sent_feat = process_sentence(sentences[j])\n",
    "                    features_all = features_all.append(sent_feat)\n",
    "                    if len(cand_sent) != 0:\n",
    "                        for k in range(0, len(cand_sent)):\n",
    "                            candidates.append(cand_sent[k])\n",
    "                            labels_all.append(labels[k])\n",
    "    \n",
    "    features_all.reset_index(drop=True, inplace=True)\n",
    "    pd.Series(labels_all).to_csv('results/'+type_file+'/labels.csv', sep=',')\n",
    "    pd.Series(candidates).to_csv('results/'+type_file+'/candidates.csv', sep=',')\n",
    "    features_all.to_csv('results/'+type_file+'/features.csv', sep=',')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('Start with training data in set I.\\n')\n",
    "    read_files('train') #debug/Q\n",
    "    print('Continue with testing data in set J.\\n')\n",
    "    read_files('test') \n",
    "    print('Finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
